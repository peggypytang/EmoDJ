{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EmoDJ Music Player\n",
    "\n",
    "EmoDJ is a brand-new AI-powered offline music player desktop application that focuses on improving listeners' emotional wellness.\n",
    "\n",
    "This application is designed based on psychology theories. It is powered by machine learning to automatically identify music emotion of your songs.\n",
    "\n",
    "To start EmoDJ at first time, click Cell>Run All<br>\n",
    "To restart EmoDJ after quit, click Kernel>Restart and Run All\n",
    "\n",
    "Supported music file format: .wav.\n",
    "Sample music files in /musics folder are downloaded from Free Music Archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music Emotion Recognition Engine\n",
    "Load trained models and predict arousal and valence value of the music <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import sklearn\n",
    "        \n",
    "def preprocess_feature(file_name):\n",
    "    n_mfcc = 12\n",
    "    mfcc_all = []\n",
    "    #MFCC per time period (500ms)\n",
    "    x, sr = librosa.load(MUSIC_FOLDER + file_name)\n",
    "    for i in range(0, len(x), int(sr*500/1000)):\n",
    "        x_cont = x[i:i+int(sr*500/1000)]\n",
    "        mfccs = librosa.feature.mfcc(x_cont,sr=sr,n_mfcc=n_mfcc)\n",
    "        #append feature value for music interval shorter than 500ms\n",
    "        mfccs = np.hstack((mfccs, np.zeros((12,22 - mfccs.shape[1]))))\n",
    "        mfccs = mfccs.flatten()\n",
    "        mfcc_all.append(mfccs)\n",
    "    return np.vstack(mfcc_all)\n",
    "\n",
    "def normalise(input_data, feature_matrix_mean, feature_matrix_std):\n",
    "    return (input_data - feature_matrix_mean) / feature_matrix_std\n",
    "\n",
    "def emotion_predict(file_name):\n",
    "    #Load trained models\n",
    "    with open(MODEL_FOLDER + 'arousal_model.pkl', 'rb') as f:\n",
    "        arousal_model = pickle.load(f)\n",
    "    with open(MODEL_FOLDER + 'valence_model.pkl', 'rb') as f:\n",
    "        valence_model = pickle.load(f)\n",
    "    with open(MODEL_FOLDER + 'feature_matrix_mean.pkl', 'rb') as f:\n",
    "        feature_matrix_mean = pickle.load(f)\n",
    "    with open(MODEL_FOLDER + 'feature_matrix_std.pkl', 'rb') as f:\n",
    "        feature_matrix_std = pickle.load(f)\n",
    "    mfcc = preprocess_feature(file_name)    \n",
    "    mfcc_norm = normalise(mfcc, feature_matrix_mean, feature_matrix_std)\n",
    "    #Predict arousal value per 5 second interval\n",
    "    music_aro = arousal_model.predict(mfcc_norm).flatten()\n",
    "    #Predict valence value per 5 second interval\n",
    "    music_val = valence_model.predict(mfcc_norm).flatten()\n",
    "    return music_aro, music_val\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music Emotion Retrieval Panel\n",
    "Display music by their valence and arousal value. Colour of marker represents colour association to average music emotion of that music piece.\n",
    "\n",
    "Listeners can see annotation (song name, valence value, arousal value) of particular music piece by hovering its marker.\n",
    "\n",
    "Listeners can retrieve and play the music piece by clicking on its marker.\n",
    "\n",
    "The music currently playing is shown in yellow colour, the music played is shown in grey colour. This would be reset if the listener select new music piece to play (reconstruct the playlist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAPPY_COLOUR = 'lawngreen'\n",
    "SAD_COLOUR = 'darkblue'\n",
    "TENSE_COLOUR = 'red'\n",
    "CALM_COLOUR = 'darkcyan'\n",
    "BASE_COLOUR = 'darkgrey'\n",
    "PLAYING_COLOUR = 'gold'\n",
    "FILE_FORMAT = '.wav'\n",
    "\n",
    "#Start playing music when user pick the marker on scatter plot\n",
    "def pick_music(event):\n",
    "    ind = event.ind\n",
    "    song_id = emotion_df.iloc[ind , :][ID_FIELD].values[0]\n",
    "    start_music(song_id)\n",
    "\n",
    "#Show annotation when user hover the marker on scatter plot\n",
    "#(song name, valence value, arousal value)\n",
    "def update_annot(ind):\n",
    "    pos = scatter_panel.get_offsets()[ind[\"ind\"][0]]\n",
    "    music_annot.xy = pos\n",
    "    (x,y) = pos\n",
    "    song_id = emotion_df[(emotion_df[VAL_FIELD]==x) & (emotion_df[ARO_FIELD]==y)][ID_FIELD].values[0]\n",
    "    music_annot.set_text(get_song_name(song_id) + \\\n",
    "                        '\\nValence: '+ str(x.round(2)) + \\\n",
    "                        '\\nArousal: '+ str(y.round(2)))                    \n",
    "\n",
    "def hover_music(event):\n",
    "    vis = music_annot.get_visible()\n",
    "    if event.inaxes == ax_panel:\n",
    "        cont, ind = scatter_panel.contains(event)\n",
    "        if cont:\n",
    "            update_annot(ind)\n",
    "            music_annot.set_visible(True)\n",
    "            canvas_panel.draw_idle()\n",
    "        else:\n",
    "            if vis:\n",
    "                music_annot.set_visible(False)\n",
    "                canvas_panel.draw_idle()\n",
    "\n",
    "#List marker colour for marks on scatter plot \n",
    "#based on corresponding emotion of average arousal valence value of that music\n",
    "def list_colour_panel(x_list, y_list):\n",
    "    colour_list = []\n",
    "    for x, y in zip(x_list,y_list):\n",
    "        if x >= 0 and y > 0:\n",
    "            colour_list.append(HAPPY_COLOUR)\n",
    "        elif x <= 0 and y < 0:\n",
    "            colour_list.append(SAD_COLOUR)\n",
    "        elif x < 0 and y >= 0:\n",
    "            colour_list.append(TENSE_COLOUR)\n",
    "        else:\n",
    "            colour_list.append(CALM_COLOUR)\n",
    "    return colour_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music Visualisation Engine\n",
    "While playing the music, Fast Fourier Transform was performed on each 1024 frames to show amplitude (converted to dB) and frequency.<br>\n",
    "Colour of line represents colour association to time vary music emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Initialise visualition\n",
    "def init_vis(): \n",
    "    line.set_ydata([0] * len(vis_x))\n",
    "    return line,\n",
    "\n",
    "#Update the visualisation\n",
    "#Line plot value based on real FFT (converted to dB)\n",
    "#Line colour based on emotion of arousal valence value at that time period\n",
    "def animate_vis(i):  \n",
    "    global num_CHUNK\n",
    "    #Show visualisation when\n",
    "    #-music file is loaded\n",
    "    #-is playing (not paused)\n",
    "    #-the music file has not finished playing\n",
    "    if wf is not None and isplay and wf.getnframes()-CHUNK*num_CHUNK>0:\n",
    "        num_CHUNK += 1\n",
    "        data = wf.readframes(CHUNK)\n",
    "        audio_data = np.frombuffer(data, np.int16)\n",
    "        dfft = 10.*np.log10(abs(np.fft.rfft(audio_data)+1)) # +1 to avoid log0\n",
    "        line.set_xdata(np.arange(len(dfft))*10.)\n",
    "        line.set_ydata(dfft)\n",
    "        line.set_color(colour_vis.pop(0))\n",
    "    else:   \n",
    "        line.set_xdata(vis_x)\n",
    "        line.set_ydata([0] * len(vis_x))\n",
    "    return line,\n",
    "\n",
    "#List colour for marks on scatter plot \n",
    "#Based on corresponding emotion of arousal valence value across time period\n",
    "def list_colour_vis(song_id):\n",
    "    global colour_vis\n",
    "    colour_vis = []\n",
    "    valence_list = valence_df[valence_df[ID_FIELD]==song_id][VAL_FIELD].values[0]\n",
    "    arousal_list = arousal_df[arousal_df[ID_FIELD]==song_id][ARO_FIELD].values[0]\n",
    "    for x, y in zip(valence_list,arousal_list):\n",
    "        if x >= 0 and y > 0:\n",
    "            colour_vis.extend([HAPPY_COLOUR]*int(TIME_PERIOD*(RATE/CHUNK)))\n",
    "        elif x <= 0 and y < 0:\n",
    "            colour_vis.extend([SAD_COLOUR]*int(TIME_PERIOD*(RATE/CHUNK)))\n",
    "        elif x < 0 and y >= 0:\n",
    "            colour_vis.extend([TENSE_COLOUR]*int(TIME_PERIOD*(RATE/CHUNK)))\n",
    "        else:\n",
    "            colour_vis.extend([CALM_COLOUR]*int(TIME_PERIOD*(RATE/CHUNK)))\n",
    "\n",
    "colour_vis = []\n",
    "num_CHUNK = 0\n",
    "TIME_PERIOD = 5 #5second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music Recommendation and Player Engine\n",
    "In addition to standard functions (such as next, pause, resume), it provides recommended playlist based on similarity of music emotion with the music selection.\n",
    "\n",
    "It would play the next music piece in playlist automatically, starting from the most similar one, until it reaches the end of playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from tkinter import messagebox\n",
    "import pygame\n",
    "\n",
    "def get_song_name(song_id):\n",
    "    return processed_music[processed_music[ID_FIELD]==song_id][NAME_FIELD].values[0]\n",
    "\n",
    "def get_song_file_path(song_id):\n",
    "    return MUSIC_FOLDER +  get_song_name(song_id)\n",
    "\n",
    "#Construct playlist based on similarity with song selected\n",
    "#Euclidean distance by valence and arousal value (square root is ignored)\n",
    "def construct_playlist(song_id):\n",
    "    global playlist\n",
    "    playlist = []\n",
    "    playlist_dict = {}\n",
    "    curr_val = emotion_df[emotion_df[ID_FIELD]==song_id][VAL_FIELD].values[0]\n",
    "    curr_aro = emotion_df[emotion_df[ID_FIELD]==song_id][ARO_FIELD].values[0]\n",
    "    song_list = list(emotion_df[ID_FIELD].values)\n",
    "    song_list.remove(song_id)\n",
    "    for compare_song_id in song_list:\n",
    "        compare_val = emotion_df[emotion_df[ID_FIELD]==compare_song_id][VAL_FIELD].values[0]\n",
    "        compare_aro = emotion_df[emotion_df[ID_FIELD]==compare_song_id][ARO_FIELD].values[0]\n",
    "        playlist_dict[compare_song_id] = (curr_val-compare_val)**2 + (curr_aro-compare_aro)**2\n",
    "    playlist_dict = sorted(playlist_dict.items(), key = lambda kv:(kv[1], kv[0]))\n",
    "    playlist = [i[0] for i in playlist_dict]\n",
    "\n",
    "#Update setting to play song \n",
    "def update_music_setting(song_id):\n",
    "    global wf, num_CHUNK\n",
    "    mixer.music.load(get_song_file_path(song_id))\n",
    "    mixer.music.play()\n",
    "    wf = wave.open(get_song_file_path(song_id), 'rb')\n",
    "    songLabel.set(get_song_name(song_id))\n",
    "    list_colour_vis(song_id)\n",
    "    ax_panel.scatter(emotion_df[VAL_FIELD],emotion_df[ARO_FIELD], s=15,c=scatter_colour,picker=False)\n",
    "    #Played songs are displayed as BASE_COLOUR\n",
    "    ax_panel.scatter(emotion_df[emotion_df[ID_FIELD].isin(played_songs)][VAL_FIELD], \\\n",
    "                     emotion_df[emotion_df[ID_FIELD].isin(played_songs)][ARO_FIELD], s=16,c=BASE_COLOUR,picker=False)\n",
    "    #Playing songs are displayed as PLAYING_COLOUR\n",
    "    ax_panel.scatter(emotion_df[emotion_df[ID_FIELD]==song_id][VAL_FIELD], \\\n",
    "                     emotion_df[emotion_df[ID_FIELD]==song_id][ARO_FIELD], s=17,c=PLAYING_COLOUR,picker=False)\n",
    "    canvas_panel.draw()\n",
    "    played_songs.append(song_id)\n",
    "    num_CHUNK = 0\n",
    "\n",
    "#User selected in panel to start song and construct new playlist\n",
    "def start_music(song_id):\n",
    "    global isplay, played_songs\n",
    "    mixer.music.stop()\n",
    "    #Construct playlist\n",
    "    construct_playlist(song_id)\n",
    "    played_songs = []\n",
    "    #Load and play song selected\n",
    "    isplay = True\n",
    "    update_music_setting(song_id)\n",
    "\n",
    "#User clicked next button to play next song in playlist\n",
    "def next_music():\n",
    "    global wf, isplay\n",
    "    mixer.music.stop()\n",
    "    if playlist:\n",
    "        isplay = True\n",
    "        song_id = playlist.pop(0)\n",
    "        update_music_setting(song_id)\n",
    "    else:\n",
    "        wf = None\n",
    "        isplay = False\n",
    "        messagebox.showinfo('EmoDJ', 'Reach the end of playlist.')\n",
    "    \n",
    "#User clicked pause/resume button  \n",
    "def pause_music():\n",
    "    global isplay\n",
    "    if wf is None and isplay:\n",
    "        messagebox.showinfo('EmoDJ', 'Please select music in Emotion Panel.')\n",
    "    elif wf:\n",
    "        if isplay:\n",
    "            isplay = False\n",
    "            mixer.music.pause()\n",
    "        else:\n",
    "            isplay = True\n",
    "            mixer.music.unpause()\n",
    "\n",
    "#Auto play next music in playlist\n",
    "def auto_next_music():\n",
    "    global isplay, wf\n",
    "    #Check if the song completes\n",
    "    if isplay:\n",
    "        if wf is not None:\n",
    "            if wf.getnframes()-CHUNK*num_CHUNK<=0:\n",
    "                #End playing if playlist exhaust\n",
    "                if playlist == [] :\n",
    "                    isplay = False\n",
    "                    wf = None\n",
    "                    songLabel.set('')\n",
    "                    messagebox.showinfo('EmoDJ', 'Reach the end of playlist.')\n",
    "                #Play next song in playlist\n",
    "                elif isplay: \n",
    "                    wf = None\n",
    "                    song_id = playlist.pop(0)\n",
    "                    update_music_setting(song_id)\n",
    "    window.after(2000, auto_next_music)\n",
    "        \n",
    "wf = None\n",
    "played_songs = []\n",
    "playlist = []\n",
    "isnext = False    \n",
    "isplay = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Index\n",
    "Create below search index files\n",
    "- Average emotion of musics \n",
    "- Time varying valence values of musics \n",
    "- Time varying arousal values of musics \n",
    "- Processed music (Music pieces with music emotion recognised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time, sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "def update_progress(processed, total):\n",
    "    bar_length = 20\n",
    "    progress = processed/total\n",
    "    \n",
    "    if isinstance(progress, int):\n",
    "        progress = float(progress)\n",
    "    if not isinstance(progress, float):\n",
    "        progress = 0\n",
    "    if progress < 0:\n",
    "        progress = 0\n",
    "    if progress >= 1:\n",
    "        progress = 1\n",
    "\n",
    "    block = int(round(bar_length * progress))\n",
    "\n",
    "    clear_output(wait = True)\n",
    "    text = \"Music Emotion Recognition Progress: [{0}] {1} / {2} songs processed\".format( \"#\" * block + \"-\" * (bar_length - block), processed, total )\n",
    "    print(text)\n",
    "    \n",
    "    \n",
    "def create_index(emotion_df, arousal_df, valence_df, processed_music, music_files):\n",
    "    #Remove music from processed music if it is not in the folder anymore\n",
    "    musics_remove = set(processed_music[NAME_FIELD].values) - set(music_files)\n",
    "    for music_name in musics_remove:\n",
    "        song_id = processed_music[processed_music[NAME_FIELD]==music_name][ID_FIELD].values[0]\n",
    "        processed_music = processed_music[processed_music.song_id != song_id]\n",
    "        emotion_df = emotion_df[emotion_df.song_id != song_id]\n",
    "        arousal_df = arousal_df[arousal_df.song_id != song_id]\n",
    "        valence_df = valence_df[valence_df.song_id != song_id]\n",
    "    \n",
    "    #Process unprocessed musics in folder\n",
    "    #Only process .wav files\n",
    "    musics_new =  set(music_files) - set(processed_music[NAME_FIELD].values)\n",
    "    num_proceeded = 0\n",
    "    \n",
    "    for music_name in musics_new: \n",
    "        update_progress(num_proceeded, len(musics_new))\n",
    "\n",
    "        if music_name.find(FILE_FORMAT)>-1:\n",
    "            music_aro, music_val = emotion_predict(music_name)\n",
    "            new_song_id = int(np.nanmax([processed_music[ID_FIELD].max(),0])) +1\n",
    "            processed_music = processed_music.append({ID_FIELD:new_song_id,NAME_FIELD:music_name}, ignore_index=True)\n",
    "            arousal_df = arousal_df.append({ID_FIELD:new_song_id,ARO_FIELD:music_aro}, ignore_index=True)\n",
    "            valence_df = valence_df.append({ID_FIELD:new_song_id,VAL_FIELD:music_val}, ignore_index=True)\n",
    "            emotion_df = emotion_df.append({ID_FIELD:new_song_id,VAL_FIELD:music_val.mean(),ARO_FIELD:music_aro.mean()}, ignore_index=True)\n",
    "        num_proceeded += 1\n",
    "        \n",
    "    #Save index\n",
    "    with open(INDEX_FOLDER+'average_emotion.pkl', 'wb') as f:\n",
    "        pickle.dump(emotion_df,f)\n",
    "    with open(INDEX_FOLDER+'arousal.pkl', 'wb') as f:\n",
    "        pickle.dump(arousal_df,f)\n",
    "    with open(INDEX_FOLDER+'valence.pkl', 'wb') as f:\n",
    "        pickle.dump(valence_df,f)\n",
    "    with open(INDEX_FOLDER+'processed_music.pkl', 'wb') as f:\n",
    "        pickle.dump(processed_music,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Index\n",
    "Load indexes if any. Otherwise, create index folder and empty indexes.<br>\n",
    "Folder structure:\n",
    "- musics/ (Music files)\n",
    "- index/ (Index files)\n",
    "- model/ (Music emotion recognition model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "MUSIC_FOLDER = 'musics/'\n",
    "INDEX_FOLDER = 'index/'\n",
    "MODEL_FOLDER = 'model/'\n",
    "VAL_FIELD = 'valence'\n",
    "ARO_FIELD = 'arousal'\n",
    "NAME_FIELD = 'song_name'\n",
    "ID_FIELD = 'song_id'\n",
    "\n",
    "def load_index():\n",
    "    #For first time using this program\n",
    "    #Create initial index\n",
    "    if not os.path.exists(INDEX_FOLDER):\n",
    "        os.makedirs(INDEX_FOLDER)\n",
    "    #Average emotion of the music\n",
    "    try:\n",
    "        with open(INDEX_FOLDER+'average_emotion.pkl', 'rb') as f:\n",
    "            emotion_df = pickle.load(f)\n",
    "    except:\n",
    "        emotion_df = pd.DataFrame(columns=[ID_FIELD, VAL_FIELD, ARO_FIELD])\n",
    "    #Dynamic arousal values of the music\n",
    "    try:\n",
    "        with open(INDEX_FOLDER+'arousal.pkl', 'rb') as f:\n",
    "            arousal_df = pickle.load(f)\n",
    "    except:\n",
    "        arousal_df = pd.DataFrame(columns=[ID_FIELD, ARO_FIELD])\n",
    "    #Dynamic valence values of the music\n",
    "    try:\n",
    "        with open(INDEX_FOLDER+'valence.pkl','rb') as f:\n",
    "            valence_df = pickle.load(f)\n",
    "    except:\n",
    "        valence_df = pd.DataFrame(columns=[ID_FIELD, VAL_FIELD])\n",
    "    #Processed music\n",
    "    try:\n",
    "        with open(INDEX_FOLDER+'processed_music.pkl','rb') as f:\n",
    "            processed_music = pickle.load(f)\n",
    "    except:\n",
    "        processed_music = pd.DataFrame(columns=[ID_FIELD, NAME_FIELD])\n",
    "        \n",
    "    emotion_df = emotion_df.astype({ID_FIELD: int})\n",
    "    arousal_df = arousal_df.astype({ID_FIELD: int})\n",
    "    valence_df = valence_df.astype({ID_FIELD: int})\n",
    "    processed_music = processed_music.astype({ID_FIELD: int})\n",
    "    \n",
    "    music_files = os.listdir(MUSIC_FOLDER)\n",
    "    if '.DS_Store' in music_files:\n",
    "        music_files.remove('.DS_Store')\n",
    "    \n",
    "    return emotion_df, arousal_df, valence_df, processed_music, music_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GUI Engine\n",
    "Graphical user interface to interact with listener.\n",
    "\n",
    "Before launching GUI, it will check if there are unprocessed music. If so, process to get music emotion values of the unprocessed music and re-create of index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enjoy music! See you next time.\n"
     ]
    }
   ],
   "source": [
    "#Due to system specification difference\n",
    "#Parameter to ensure synchronisation of visualisation and sound \n",
    "SYNC = 3.5 \n",
    "\n",
    "import tkinter as tk\n",
    "import wave \n",
    "from pygame import mixer\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from PIL import ImageTk, Image\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "def quit_window():\n",
    "    mixer.music.stop()\n",
    "    window.withdraw()\n",
    "    window.update()\n",
    "    window.destroy()\n",
    "    \n",
    "def cmdNext(): next_music() \n",
    "def cmdPause(): pause_music()\n",
    "def cmdQuit(): quit_window()\n",
    "\n",
    "emotion_df, arousal_df, valence_df, processed_music, music_files = load_index()\n",
    "\n",
    "#Check if there are unprocessed music\n",
    "unprocessed_music = set(music_files).symmetric_difference(set(processed_music[NAME_FIELD].values))\n",
    "if unprocessed_music:\n",
    "    print('Processing musics...')\n",
    "    create_index(emotion_df, arousal_df, valence_df, processed_music, music_files)\n",
    "    \n",
    "emotion_df, arousal_df, valence_df, processed_music, _ = load_index()\n",
    "        \n",
    "window = tk.Tk()\n",
    "window.title(\"EmoDJ\")\n",
    "window.config(background='white')\n",
    "window.geometry('1300x700')\n",
    "\n",
    "#EmoDJ logo\n",
    "logo_image = ImageTk.PhotoImage(Image.open(MODEL_FOLDER + 'logo.jpeg').resize((150, 80), Image.ANTIALIAS))\n",
    "tk.Label(window, image = logo_image,background='white').grid(row = 0,column=0,sticky='W')\n",
    "\n",
    "#Song name\n",
    "songLabel = tk.StringVar()\n",
    "songLabel.set('')\n",
    "tk.Label(window, textvariable=songLabel,background='white').grid(row = 1, column=0)\n",
    "\n",
    "#music visualisation\n",
    "fig_vis, ax_vis = plt.subplots(figsize=(8,5))\n",
    "RATE = 44100\n",
    "CHUNK = 1024\n",
    "max_x = CHUNK+1\n",
    "max_y = 100\n",
    "\n",
    "vis_x = np.arange(0, max_x)\n",
    "ax_vis.set_xlim(0, max_x)\n",
    "ax_vis.set_ylim(0, max_y)\n",
    "ax_vis.set_axis_off()\n",
    "line, = ax_vis.plot(vis_x, [0] * len(vis_x))\n",
    "canvas_vis = FigureCanvasTkAgg(fig_vis, master=window)\n",
    "canvas_vis.get_tk_widget().grid(row=2,column=0,rowspan=2,sticky='W'+'E'+'N'+'S')\n",
    "ani = animation.FuncAnimation(fig_vis, animate_vis, init_func=init_vis, interval=int(math.ceil(1000/(RATE/CHUNK)))+SYNC, blit=True)\n",
    "\n",
    "#music player\n",
    "tk.Button(window, text=\"Quit\", command=cmdQuit).grid(row = 0, column=2,sticky='E'+'N')\n",
    "mixer.pre_init(frequency=RATE, size=-16, channels=2)\n",
    "mixer.init()\n",
    "tk.Button(window, text=\"Next\", command=cmdNext).grid(row = 1, column=1,sticky='W'+'E'+'N'+'S')\n",
    "tk.Button(window, text=\"Resume/Pause\", command=cmdPause).grid(row = 1, column=2,sticky='W'+'E'+'N'+'S')\n",
    "\n",
    "#music emotion panel\n",
    "fig_panel, ax_panel = plt.subplots(figsize=(5,5))\n",
    "scatter_colour = list_colour_panel(emotion_df[VAL_FIELD], emotion_df[ARO_FIELD])\n",
    "scatter_panel = ax_panel.scatter(emotion_df[VAL_FIELD],emotion_df[ARO_FIELD], s=15,c=scatter_colour, picker=True)\n",
    "ax_panel.axvline(x=0,c=BASE_COLOUR)\n",
    "ax_panel.axhline(y=0,c=BASE_COLOUR)\n",
    "ax_panel.set_xlim(-1, 1)\n",
    "ax_panel.set_ylim(-1, 1)\n",
    "ax_panel.text(-1,0.05,s='-1 Valence',fontsize=9,color=BASE_COLOUR)\n",
    "ax_panel.text(0.75,0.05,s='+1 Valence',fontsize=9,color=BASE_COLOUR)\n",
    "ax_panel.text(0.05,-1,s='-1 Arousal',fontsize=9,color=BASE_COLOUR)\n",
    "ax_panel.text(0.05,1,s='+1 Arousal',fontsize=9,color=BASE_COLOUR)\n",
    "ax_panel.set_axis_off()\n",
    "music_annot = ax_panel.annotate(\"\", xy=(0,0), xytext=(20,20),textcoords=\"offset points\",bbox=dict(boxstyle=\"round\", fc=\"w\"))\n",
    "music_annot.set_visible(False)\n",
    "canvas_panel = FigureCanvasTkAgg(fig_panel, master=window)\n",
    "canvas_panel.get_tk_widget().grid(row=2,column=1,columnspan=2,sticky='W'+'E'+'N'+'S')\n",
    "canvas_panel.mpl_connect('pick_event', pick_music)\n",
    "canvas_panel.mpl_connect(\"motion_notify_event\", hover_music)\n",
    "tk.Label(window, text=\"Start playing music by clicking on the marker!\",background='white').grid(row = 3,column=1,columnspan=2,sticky='W'+'E'+'N'+'S')\n",
    "window.after(0, auto_next_music)\n",
    "window.mainloop()\n",
    "print('Enjoy music! See you next time.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
